{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fa1705",
   "metadata": {},
   "source": [
    "# Deep Learning: Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "id": "f364fb03",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:33:14.949838Z",
     "start_time": "2024-04-23T20:33:13.384829Z"
    }
   },
   "source": [
    "# Define imports & defaults\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# import helper functions\n",
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.getcwd())\n",
    "from a02helper import *\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)  # ensure reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 32"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "237ac6da",
   "metadata": {},
   "source": [
    "## Task 1: Datasets"
   ]
  },
  {
   "cell_type": "code",
   "id": "4325298a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:43:32.815191Z",
     "start_time": "2024-04-23T20:43:32.805656Z"
    }
   },
   "source": [
    "import string\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        reviews_file=\"data/reviews_small.txt\",\n",
    "        labels_file=\"data/labels_small.txt\",\n",
    "        use_vocab=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A dataset of movie reviews and their labels.\n",
    "\n",
    "        Args:\n",
    "            reviews_file: the reviews file\n",
    "            labels_file:  the labels file\n",
    "            use_vocab: if True, yield reviews in a numerical representation\n",
    "        \"\"\"\n",
    "        # Load data from filesystem\n",
    "        with open(reviews_file) as f:\n",
    "            raw_reviews = f.readlines()\n",
    "\n",
    "        with open(labels_file) as f:\n",
    "            raw_labels = f.readlines()\n",
    "\n",
    "        # Preprocessing and store (in memory)\n",
    "        self._reviews = self._preprocess_reviews(raw_reviews)\n",
    "        self._labels = self._preprocess_labels(raw_labels)\n",
    "\n",
    "        # Build vocabulary\n",
    "        self.vocab = None\n",
    "        if use_vocab:\n",
    "            from torchtext.vocab import build_vocab_from_iterator\n",
    "            self.vocab = build_vocab_from_iterator(\n",
    "                self._reviews, specials=[\"<pad>\"]  # will get token id 0\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the length of the dataset.\"\"\"\n",
    "        return len(self._reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a tuple of a preprocessed review and the corresponding label. If the\n",
    "        vocabulary is enabled, returns a numerical representation of the review.\n",
    "\n",
    "        Args:\n",
    "            idx: a single index\n",
    "        Returns: a (review, label) tuple\n",
    "        \"\"\"\n",
    "        review = self._reviews[idx]\n",
    "        label = self._labels[idx]        \n",
    "        if self.vocab:\n",
    "            review_ids = [self.vocab.get_stoi()[token] for token in review]\n",
    "            return review_ids, label\n",
    "        else:\n",
    "            return review, label\n",
    "\n",
    "    def _preprocess_reviews(self, raw_reviews):\n",
    "        \"\"\"\n",
    "        Applies two kinds of preprocessing:\n",
    "\n",
    "        (i) Apply the \"basic_english\" tokenizer from the torchtext library to\n",
    "        transform every review into a list of normalized tokens (cf.\n",
    "        https://pytorch.org/text/stable/data_utils.html#get-tokenizer).\n",
    "\n",
    "        (ii) Remove punctuation (cf.\n",
    "        https://docs.python.org/3/library/string.html#string.punctuation).\n",
    "\n",
    "        Returns: list of tokenized reviews\n",
    "        \"\"\"\n",
    "        tokenizer = get_tokenizer(\"basic_english\")\n",
    "        tokenized_list = []\n",
    "        for review in range (len(raw_reviews)):\n",
    "            temp_str = raw_reviews[review].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "            temp_str = tokenizer(temp_str)\n",
    "            tokenized_list.append(temp_str)\n",
    "        return tokenized_list\n",
    "\n",
    "    def _preprocess_labels(self, raw_labels):\n",
    "        \"\"\"\n",
    "        Transform raw labels into integers, where 1=\"positive\" and 0 otherwise.\n",
    "\n",
    "        Returns: list of labels\n",
    "        \"\"\"\n",
    "        # Hint: You can remove leading and trailing whitespace from the raw labels using\n",
    "        # the strip() method.\n",
    "        preprocessed_labels_list = []\n",
    "        for label in raw_labels:\n",
    "            label = label.strip()\n",
    "            if label == \"positive\":\n",
    "                preprocessed_labels_list.append(1)\n",
    "            else:\n",
    "                preprocessed_labels_list.append(0)\n",
    "        return preprocessed_labels_list"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "fcdcdf34",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:43:33.236567Z",
     "start_time": "2024-04-23T20:43:33.000004Z"
    }
   },
   "source": [
    "# Test your code (without vocabulary).\n",
    "dataset = ReviewsDataset()\n",
    "print(dataset[0])\n",
    "\n",
    "# Should yield:\n",
    "# (['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', ... ], 1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['bromwell', 'high', 'is', 'a', 'cartoon', 'comedy', 'it', 'ran', 'at', 'the', 'same', 'time', 'as', 'some', 'other', 'programs', 'about', 'school', 'life', 'such', 'as', 'teachers', 'my', 'years', 'in', 'the', 'teaching', 'profession', 'lead', 'me', 'to', 'believe', 'that', 'bromwell', 'high', 's', 'satire', 'is', 'much', 'closer', 'to', 'reality', 'than', 'is', 'teachers', 'the', 'scramble', 'to', 'survive', 'financially', 'the', 'insightful', 'students', 'who', 'can', 'see', 'right', 'through', 'their', 'pathetic', 'teachers', 'pomp', 'the', 'pettiness', 'of', 'the', 'whole', 'situation', 'all', 'remind', 'me', 'of', 'the', 'schools', 'i', 'knew', 'and', 'their', 'students', 'when', 'i', 'saw', 'the', 'episode', 'in', 'which', 'a', 'student', 'repeatedly', 'tried', 'to', 'burn', 'down', 'the', 'school', 'i', 'immediately', 'recalled', 'at', 'high', 'a', 'classic', 'line', 'inspector', 'i', 'm', 'here', 'to', 'sack', 'one', 'of', 'your', 'teachers', 'student', 'welcome', 'to', 'bromwell', 'high', 'i', 'expect', 'that', 'many', 'adults', 'of', 'my', 'age', 'think', 'that', 'bromwell', 'high', 'is', 'far', 'fetched', 'what', 'a', 'pity', 'that', 'it', 'isn', 't'], 1)\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "id": "ebcd9318",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-23T20:43:35.018375Z",
     "start_time": "2024-04-23T20:43:33.507299Z"
    }
   },
   "source": [
    "# Test your code (with vocabulary).\n",
    "dataset = ReviewsDataset(use_vocab=True)\n",
    "print(dataset[0])\n",
    "\n",
    "# Should yield:\n",
    "# ([10661, 307, 6, 3, 1177, 202, 8,  ... ], 1)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([10661, 307, 6, 3, 1177, 202, 8, 2248, 33, 1, 168, 56, 15, 49, 85, 8902, 43, 422, 122, 140, 15, 3234, 59, 144, 9, 1, 5504, 6267, 454, 72, 5, 260, 12, 10661, 307, 13, 2060, 6, 73, 2780, 5, 692, 76, 6, 3234, 1, 29527, 5, 1730, 7117, 1, 6161, 1726, 36, 52, 68, 212, 143, 63, 1409, 3234, 17974, 1, 28056, 4, 1, 221, 758, 31, 2748, 72, 4, 1, 6311, 10, 731, 2, 63, 1726, 54, 10, 208, 1, 321, 9, 64, 3, 1601, 4042, 743, 5, 2853, 187, 1, 422, 10, 1254, 10116, 33, 307, 3, 380, 322, 6162, 10, 135, 136, 5, 10172, 30, 4, 134, 3234, 1601, 2545, 5, 10661, 307, 10, 529, 12, 113, 1841, 4, 59, 676, 103, 12, 10661, 307, 6, 227, 4163, 48, 3, 2201, 12, 8, 231, 21], 1)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "025e9e65",
   "metadata": {},
   "source": [
    "## Task 2: Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2dc4e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Split dataset into training, validation, and test subsets\n",
    "dataset = ReviewsDataset(use_vocab=True)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(\n",
    "    dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(123)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51540bfb",
   "metadata": {},
   "source": [
    "### Task 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f2213",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Example usage of a data loader\n",
    "dataloader = DataLoader(\n",
    "    val_set,  # a dataset\n",
    "    1,  # desired batch size\n",
    "    False,  # whether to randomly shuffle the dataset\n",
    "    num_workers=1,  # number of workers that construct batches in parallel\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe7905a",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's print the first batch\n",
    "batch = next(iter(dataloader))\n",
    "print(batch)\n",
    "\n",
    "# [[tensor([11]), tensor([6]), tensor([1]), ...], tensor([0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7160793e",
   "metadata": {},
   "source": [
    "### Task 2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91c149f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def review_collate_fn(raw_batch):\n",
    "    \"\"\"Prepare batches of reviews from a review dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_batch: collection of (review, label)-tuples from a ReviewDataset\n",
    "\n",
    "    Returns: a tuple (review x token id tensor, label tensor) of sizes\n",
    "    batch_size*MAX_SEQ_LEN and batch_size, respectively.\n",
    "\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b88d4b",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your function\n",
    "review_collate_fn([([1, 2, 3], 1), (torch.arange(MAX_SEQ_LEN * 2) + 1, 0)])\n",
    "\n",
    "# Should yield:\n",
    "# (tensor([[  1,   2,   3,   0,   0,  ..., 0 ],\n",
    "#          [  1,   2,   3,   4,   5, ..., 200 ]]),\n",
    "#  tensor([1, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51299831",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create the data loaders (with shuffling for training data -> randomization)\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, True, collate_fn=review_collate_fn)\n",
    "val_loader = DataLoader(val_set, BATCH_SIZE, False, collate_fn=review_collate_fn)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE, False, collate_fn=review_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6092fc9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's print the first batch\n",
    "batch = next(iter(val_loader))\n",
    "print(batch)\n",
    "\n",
    "\n",
    "# (tensor([[   11,     6,     1,  ...,     0,     0,     0],\n",
    "#         [   11,   170,  2220,  ...,     0,     0,     0],\n",
    "#         [   48,     3, 30376,  ...,     0,     0,     0],\n",
    "#         ...,\n",
    "#         [  176,    56,    10,  ...,     0,     0,     0],\n",
    "#         [  239,   534,  1404,  ...,    44,   120,     1],\n",
    "#         [ 2954, 15576,     6,  ...,  2678,    65,     1]]),\n",
    "#  tensor([0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1,\n",
    "#         0, 0, 0, 1, 1, 1, 0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04630d",
   "metadata": {},
   "source": [
    "## Task 3: Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bcc6e0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self, vocab_size, embedding_dim, hidden_dim, num_layers=1, cell_dropout=0.0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the model by setting up the layers.\n",
    "\n",
    "        Args:\n",
    "            vocab_size: number of unique words in the reviews\n",
    "            embedding_dim: size of the embeddings\n",
    "            hidden_dim: dimension of the LSTM output\n",
    "            num_layers: number of LSTM layers\n",
    "            cell_dropout: dropout applied between the LSTM layers\n",
    "                          (provide to LSTM constructor as dropout argument)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of the model on some input and hidden state.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: batch as a (batch_size, sequence_length) tensor\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Probability of positive class and the last output of the LSTM.\n",
    "        \"\"\"\n",
    "        # init hidden layer, which is needed for the LSTM\n",
    "        hidden = self.init_hidden(len(x))\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden states.\n",
    "\n",
    "        Returns a tuple of two num_layers x batch_size x hidden_dim tensors (one for\n",
    "        initial cell states, one for initial hidden states) consisting of all zeros.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        # Note: ensure that the returned tensors are located on DEVICE.\n",
    "        pass\n",
    "\n",
    "\n",
    "# Test constructor\n",
    "model = SimpleLSTM(50, 10, 32, 2, 0.1).to(DEVICE)\n",
    "print(model)\n",
    "\n",
    "# Should give:\n",
    "# SimpleLSTM(\n",
    "#   (embedding): Embedding(50, 10)\n",
    "#   (lstm): LSTM(10, 32, num_layers=2, batch_first=True, dropout=0.1)\n",
    "#   (fc): Linear(in_features=32, out_features=1, bias=True)\n",
    "#   (sigmoid): Sigmoid()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b610db5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test forward pass\n",
    "model = SimpleLSTM(50, 10, 32, 2).to(DEVICE)\n",
    "dummy_data = torch.arange(30, dtype=torch.int, device=DEVICE).reshape(3, 10)\n",
    "\n",
    "# fix model parameters\n",
    "for key in model.state_dict():\n",
    "    model.state_dict()[key][:] = 0.1\n",
    "probs, states = model(dummy_data)\n",
    "print(probs)\n",
    "print(states)\n",
    "\n",
    "\n",
    "# tensor([[0.9643],\n",
    "#         [0.9643],\n",
    "#         [0.9643]], device='cuda:0 or cpu', grad_fn=<SigmoidBackward0>)\n",
    "# tensor([[0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985],\n",
    "#         [0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985],\n",
    "#         [0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985, 0.9985,\n",
    "#          0.9985, 0.9985, 0.9985, 0.9985, 0.9985]], device='cuda:0 or cpu',\n",
    "#        grad_fn=<SliceBackward0>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b61465b",
   "metadata": {},
   "source": [
    "### Task 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e32c14d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # Disables autograd for this function\n",
    "def reviews_eval(\n",
    "    model, eval_loader, label=\"val\", loss_fn=torch.nn.functional.binary_cross_entropy\n",
    "):\n",
    "    model.to(DEVICE)\n",
    "    model.eval()  # sets model to evaluation mode (e.g., relevant for dropout)\n",
    "\n",
    "    total_correct = total_loss = 0\n",
    "    for reviews, labels in eval_loader:\n",
    "        reviews, labels = reviews.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        # Forward pass: Compute the model's output, reshape it to a vector, and\n",
    "        # then run the provided loss function.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Eval stats: Add loss to total_loss and number of correct predictions to\n",
    "        # total_correct.\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    print(\n",
    "        f\"            \"\n",
    "        f\"{label} accuracy: {total_correct / len(eval_loader.dataset):.4f}\\t\"\n",
    "        f\"{label} loss: {total_loss / len(eval_loader):.4f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db972e44",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "model = SimpleLSTM(len(dataset.vocab), 10, 10, 1, 0).to(DEVICE)\n",
    "reviews_eval(model, val_loader)\n",
    "\n",
    "# Should yield with different but similar numbers:\n",
    "#             val accuracy: 0.5100\tval loss: 0.6928"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc6ec6",
   "metadata": {},
   "source": [
    "### Task 3e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288bbb97",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reviews_train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    lr=0.01,\n",
    "    epochs=3,\n",
    "    max_norm=5,\n",
    "    loss_fn=torch.nn.functional.binary_cross_entropy,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a network on the review data\n",
    "\n",
    "    Args:\n",
    "        model: Initialized model\n",
    "        train_loader: Dataloader for the training data\n",
    "        val_loader: Dataloader for the validation data\n",
    "        lr: learning rate\n",
    "        epochs: number of epochs\n",
    "        max_norm: max norm of gradients for gradient clipping\n",
    "        loss_fn: Loss function\n",
    "    \"\"\"\n",
    "    # Send the model's parameters to your accelerator (cuda or cpu)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Define optimizer for the parameters which require gradients (cf. Task 5)\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [param for param in model.parameters() if param.requires_grad], lr=lr\n",
    "    )\n",
    "\n",
    "    # Let's go\n",
    "    for epoch in range(epochs):\n",
    "        total_correct = total_loss = 0\n",
    "        for reviews, labels in train_loader:\n",
    "            model.train()\n",
    "\n",
    "            # Send batch to your accelerator\n",
    "            reviews, labels = reviews.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            # Forward pass: Compute the model's output, reshape it to a vector, and then\n",
    "            # run the provided loss function.\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # Backward pass:\n",
    "            # (i)   Compute the gradients wrt. the loss\n",
    "            # (ii)  Clip the gradients using\n",
    "            #       https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html to max_norm\n",
    "            # (iii) Run the optimizer\n",
    "            # (iv)  Clear all accumulated gradients\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "            # Compute epoch statistics:\n",
    "            # (i)  Add the loss of this batch to the total_loss\n",
    "            # (ii) Add the number of correct predictions (max prob) to total_correct\n",
    "            # YOUR CODE HERE\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1:2}\\t\"\n",
    "            f\"train accuracy: {total_correct / len(train_loader.dataset):.4f}\\t\"\n",
    "            f\"train loss: {total_loss / len(train_loader):.4f}\"\n",
    "        )\n",
    "\n",
    "        # now validate\n",
    "        reviews_eval(model, val_loader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e21533",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test your implementation\n",
    "model = SimpleLSTM(len(dataset.vocab), 10, 10, 1).to(DEVICE)\n",
    "reviews_train(model, train_loader, val_loader, epochs=5)\n",
    "\n",
    "# Should yield something like (note: numbers have high variance over runs):\n",
    "# Epoch  1\ttrain accuracy: 0.4994\ttrain loss: 0.6953\n",
    "#             val accuracy: 0.4875\tval loss: 0.6922\n",
    "# Epoch  2\ttrain accuracy: 0.5319\ttrain loss: 0.6885\n",
    "#             val accuracy: 0.5275\tval loss: 0.6861\n",
    "# Epoch  3\ttrain accuracy: 0.6059\ttrain loss: 0.6443\n",
    "#             val accuracy: 0.5400\tval loss: 0.6902\n",
    "# Epoch  4\ttrain accuracy: 0.6863\ttrain loss: 0.5438\n",
    "#             val accuracy: 0.5925\tval loss: 0.7453\n",
    "# Epoch  5\ttrain accuracy: 0.8334\ttrain loss: 0.3875\n",
    "#             val accuracy: 0.7300\tval loss: 0.6310"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b9cc4",
   "metadata": {},
   "source": [
    "## Task 4: Pre-trained Embeddings & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1c8ce5",
   "metadata": {},
   "source": [
    "### Task 4b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0d4516",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Glove embeddings into a plain embedding layer.\n",
    "vocab = dataset.vocab\n",
    "glove_embeddings = nn.Embedding(len(vocab), 100, device=DEVICE)\n",
    "reviews_load_embeddings(glove_embeddings, vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694aece7",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Print one embedding\n",
    "glove_embeddings(torch.tensor(vocab[\"movie\"], device=DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f783e3f8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot embeddings of first 100 words using t-SNE\n",
    "nextplot()\n",
    "_ = tsne_vocab(glove_embeddings, torch.arange(100), vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531b640",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# You can also specify colors and/or drop the item labels\n",
    "nextplot()\n",
    "_ = tsne_vocab(glove_embeddings, torch.arange(100), colors=[0] * 50 + [1] * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210a33e8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Note: you can obtain the embeddings tensor using glove_embeddings.weight.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6438e5",
   "metadata": {},
   "source": [
    "### Task 4c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91dbc24",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# hyperparameter settings for rest of task 4\n",
    "vocab_size = len(dataset.vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 100\n",
    "num_layers = 2\n",
    "n_epochs = 10\n",
    "cell_dropout = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09c5d6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train a plain model\n",
    "model = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, cell_dropout).to(\n",
    "    DEVICE\n",
    ")\n",
    "reviews_train(model, train_loader, val_loader, epochs=n_epochs)\n",
    "\n",
    "# Should reach a (train) accuracy of >0.9. If not, rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1235b7f4",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot t-SNE embeddings of the thought vectors for training data\n",
    "# point color = label\n",
    "nextplot()\n",
    "_ = tsne_thought(model, train_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18c68dd",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot t-SNE embeddings of of the thought vectors for validation data\n",
    "nextplot()\n",
    "_ = tsne_thought(model, val_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cab379",
   "metadata": {},
   "source": [
    "### Task 4d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27fa220",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model with *p*re-trained embeddings with *f*inetuning, then train\n",
    "model_pf = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, cell_dropout)\n",
    "reviews_load_embeddings(model_pf.embedding, vocab.get_stoi())\n",
    "reviews_train(model_pf, train_loader, val_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fea5029",
   "metadata": {},
   "source": [
    "### Task 4e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925c415",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize the model with *p*re-trained embeddings without finetuning, then train\n",
    "model_p = SimpleLSTM(vocab_size, embedding_dim, hidden_dim, num_layers, cell_dropout)\n",
    "reviews_load_embeddings(model_p.embedding, vocab.get_stoi())\n",
    "model_p.embedding.weight.requires_grad = False\n",
    "reviews_train(model_p, train_loader, val_loader, epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2c3b7",
   "metadata": {},
   "source": [
    "## Task 5: Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d817852d",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
